{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Машинное обучение с подкреплением. МФТИ: МТИИ 2020-2021.\n", "\n", "## Семинар 4: Предобучение навыков (опций) \n", "Нашей задачей будет создание набора опций, каждая из которых должна быть обучена достигать определенные состояния в задаче такси. Для обучения мы будем использовать QLearningAgent, которого мы написали на одном из прошлых семинаров. "]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["# импортируем файлы и создаем окружение\n", "import gym\n", "import random\n", "import numpy as np\n", "\n", "environment = gym.make('Taxi-v3')\n", "environment.render()\n"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["# импортируем классс Q-агента из прошлого занятия\n", "from q_agent import QLearningAgent"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Задание 1 \n", "Разберемся как реализована среда Taxi: https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py\n", "\n", "Создадим 4 окружения аналогичных Taxi, в которых целью агента будет достижение одной из точек: R, G, B, Y соответственно. "]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["class TaxiStepWrapper(gym.Wrapper):\n", "    def __init__(self, env, target_id, target_reward):\n", "        super().__init__(env)\n", "        self._target = target_id\n", "        self._target_reward = target_reward\n", "\n", "    def _step(self, action):\n", "        # получаем параметры (state, reward, _, obs),\n", "        # которые передает среда, используя метод step \n", "        # проверяем является ли состояние завершающим\n", "        # для нашего модифицированного окружения\n", "        # изменяем вознаграждение (reward) и \n", "        # флаг завершения эпизода (is_done)\n", "        # за каждое действие будем давать вознаграждение \n", "        # -1, за достижение цели - self._target_reward\n", "        ####### Здесь ваш код ########     \n", "        raise NotImplementedError     \n", "        ##############################\n", "        \n", "\n", "        return state, reward, is_done, obs\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Проверим нашу обертку (wrapper), используя случайную стратегию.  Порядок точек должен быть  R, G, Y, B."]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["for target in range(4):\n", "    # создаем окружение с заданным целевым состоянием\n", "    ####### Здесь ваш код ########\n", "    raise NotImplementedError\n", "    ##############################\n", "    \n", "    \n", "    # применяем случайную стратегию,\n", "    # пока эпизод не завершится\n", "    ####### Здесь ваш код ########\n", "    raise NotImplementedError\n", "    ##############################\n", "    \n", "\n", "    wrapped_env.render()\n", "    print(\"state:{s} reward:{r}\\n\".format(**locals()))\n"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["# воспользуемся методом play_and_train, \n", "# который мы реализовали на прошлом семинаре\n", "def play_and_train(env, agent, t_max=10 ** 4):\n", "    total_discounted_reward = 0.0\n", "    s = env.reset()\n", "    for t in range(t_max):\n", "        a = agent.get_action(s)\n", "        next_s, r, done, _ = env.step(a)\n", "        agent.update(s, a, next_s, r)\n", "        s = next_s\n", "        total_discounted_reward += r\n", "        if done:\n", "            break\n", "    return total_discounted_reward\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Задание 2 \n", "1. Обучим агентов на созданных нами окружениях.\n", "2. Создадим упрощенный вариант опций, каждая опция будет иметь стратегию, множество начальных состояний и множество конечных состояний."]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["n_actions = environment.action_space.n\n", "\n", "# параметры, которые будут использовать агенты\n", "params = {\"alpha\": 0.1, \"epsilon\": 0.1, \n", "\"gamma\": 0.99, \"get_legal_actions\": lambda s: range(4)}\n", "\n", "# создаем агентов \n", "agents_for_options = [QLearningAgent(**params)\\\n", "                      for _ in range(4)]\n", "\n", "for index in range(4):\n", "    # создаем окружение с заданным целевым состоянием, \n", "    # используя созданных окружения обучаем агентов\n", "    ####### Здесь ваш код ########\n", "    raise NotImplementedError\n", "    ##############################\n", "    \n"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["# реализуем класс опции\n", "class Option:\n", "    def __init__(self, policy, termination_prob, initial):\n", "        self.policy = policy\n", "        self.termination_prob = termination_prob\n", "        self.initial_states = initial\n", "\n", "    def can_start(self, state):\n", "        return state in self.initial_states\n", "\n", "    def terminate(self, state):\n", "        return random.random() <= self.termination_prob[\n", "            state]\n", "\n", "    def get_action(self, state):\n", "        return self.policy.get_action(state)"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["options = []\n", "for index, agent in enumerate(agents_for_options):\n", "    # Создаем словарь termination_prob, в котором каждому \n", "    # состоянию  нужно задать вероятность завершения \n", "    # опции. В нашем случае зададим 1.0 или 0.0, \n", "    # в зависимости от состояния.\n", "    # Создаем множество initial, добавляем в него \n", "    # состояния, из которых опция может быть \n", "    # вызвана (все кроме целевых)\n", "    termination_prob = {}\n", "    initial_states = set()\n", "    termination_states = set()\n", "    \n", "    ####### Здесь ваш код ########\n", "    raise NotImplementedError\n", "    ##############################\n", "    \n", "\n", "    options.append(Option(policy=agent, \\\n", "    termination_prob=termination_prob, initial=initial))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Задание 3\n", "Напишем функцию, которая будет запускать опцию и возвращать дисконтированное вознаграждение, опираясь на число совершенных действий\n", "$$ R = r_{1} + \\gamma r_{2} + \\gamma^{2} r_{3} + \\dots + \\gamma^{t-1}r_{t}$$"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["def apply_option(option, gamma, env, debug=False):\n", "    reward = 0\n", "    steps = 0\n", "\n", "    if not option.can_start(env.unwrapped.s):\n", "        raise KeyError\n", "    \n", "    # Взаимодействуем со средой пока опция или окружение \n", "    # не завершится, считаем дисконтированное \n", "    # вознаграждение reward (используем steps),\n", "    # также добавим render окружения, при флаге - debug\n", "    \n", "    ####### Здесь ваш код ########\n", "    raise NotImplementedError\n", "    ##############################\n", "    \n", "    # state, reward, is_done, obs\n", "    return s, reward, d, obs\n"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["# проверим работу метода\n", "env = gym.make('Taxi-v3')\n", "s = env.reset()\n", "\n", "r = apply_option(options[0], 0.99, env, debug=True)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Кажется, что все хорошо, но мы забыли рассмотреть вариант, когда пассажир может находиться в такси! Переведем среду в состояние, где пассажира мы уже подобрали и посмотрим, как ведет себя  одна из опций."]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["s = env.reset()\n", "env.unwrapped.s = 499\n", "env.render()\n", "print(\"\\n\" * 2)\n", "r = apply_option(options[0], 0.99, env, debug=True)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Задание 4\n", "Видим, что опции не обучились действовать в такой ситуации. \n", "Исправим нашу функцию обучения так, чтобы опции работали корректно для всех возможных состояний среды и сгенерируем их заново."]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": ["def play_and_train_modified(env, agent, t_max=10 ** 4):\n", "    # Зададим новую функцию play_and_train, которая \n", "    # в качестве начального состояния выбирает любое \n", "    # состояние среды,включая и то, когда пассажир \n", "    # находится в такси\n", "\n", "    total_discounted_reward = 0.0\n", "    s = env.reset()\n", "    \n", "    # Выбираем случайное состояние среды\n", "    # (используем метод env.uwrapped)\n", "    ####### Здесь ваш код ########\n", "    raise NotImplementedError\n", "    ##############################\n", "    \n", "\n", "    for t in range(t_max):\n", "        a = agent.get_action(s)\n", "        next_s, r, done, _ = env.step(a)\n", "        agent.update(s, a, next_s, r)\n", "        s = next_s\n", "        total_discounted_reward += r\n", "        if done:\n", "            break\n", "    return total_discounted_reward\n", "\n", "\n", "for index in range(4):\n", "    for _ in range(5250):\n", "        wrapped_env = TaxiStepWrapper(env=environment, \n", "                target_id=index, target_reward=50)\n", "        play_and_train_modified(env=wrapped_env, \n", "                agent=agents_for_options[index])\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Запустим данную ячейку несколько раз и убедимся, что агент обучился для всех случаев!"]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": ["env = environment\n", "s = env.reset()\n", "\n", "env.unwrapped.s = random.randint(0, 499)\n", "apply_option(options[0], 0.99, env, debug=True)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Объединение в иерархию\n", "Реализуйте иерархию, используя элементарные (опции из одного действия) и обученные опции."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Добавим элементарные опции (опции из одного действия: посадка и высадка пассажира):"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": ["# для действий 4-5 (pickup, dropoff) создаем \n", "# элементарные опции:\n", "class OneActionAgent:\n", "    def __init__(self, action):\n", "        self.action = action\n", "    \n", "    def get_action(self, state):\n", "        return self.action\n", "    \n", "    def update(*args, **kwargs):\n", "        pass\n", "\n", "options = options[:4]    \n", "for action in range(4, 6):\n", "    # элементарная опция начинается в любом состоянии, \n", "    # выполняет любое действие и завершается\n", "    initial = set(range(environment.observation_space.n))\n", "    termination_prob = {_:1.0 \\\n", "    for _ in  range(environment.observation_space.n)}   \n", "    options.append(Option(policy=OneActionAgent(action), \n", "    termination_prob=termination_prob, initial=initial))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Проверим элементарные опции:"]}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": ["env = environment\n", "s = env.reset()\n", "\n", "env.unwrapped.s = random.randint(0, 499)\n", "apply_option(options[0], 0.99, env, debug=True)\n", "apply_option(options[4], 0.99, env, debug=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Реализуем обертку для окружения, которая вместо действий применяет опции (в качестве входа - список опций):"]}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": ["class OptionTaxiStepWrapper(gym.Wrapper):\n", "    def __init__(self, env, options, gamma=0.99):\n", "        self.options = options\n", "        self.gamma = gamma\n", "        super().__init__(env)\n", "\n", "    def _step(self, action):\n", "        state, reward, is_done, obs =\\\n", "        apply_option(self.options[action],\n", "            self.gamma, self.unwrapped)\n", "        return state, reward, is_done, obs\n"]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": ["option_agent = QLearningAgent(alpha=0.1, epsilon=0.1,\n", "                              gamma=0.99,\n", "                              get_legal_actions=lambda\n", "                                  s: range(len(options)))\n", "\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "from IPython.display import clear_output\n", "\n", "# создаем окружение, использующее опции\n", "env = OptionTaxiStepWrapper(gym.make('Taxi-v3'),\n", "                            options=options)\n", "rewards = []\n", "for episode in range(5000):\n", "    rewards.append(\n", "        play_and_train(env=env, agent=option_agent))\n", "\n", "    if episode % 100 == 0:\n", "        clear_output(True)\n", "        option_agent.epsilon *= 0.99\n", "        plt.plot(rewards)\n", "        plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 2}