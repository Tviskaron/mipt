{"cells": [{"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "CpceefzmsN45"}, "source": ["# Машинное обучение с подкреплением. МФТИ: МТИИ 2020-2021.\n", "## Семинар 2: Динамическое программирование. "]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "uX-Ky982sN47"}, "source": ["Рассмотрим алгоритм итерации по оценкам состояния $V$ (Value Iteration):\n", "    $$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n", "На основе оценки $V_i$ можно посчитать функцию оценки $Q_i$ действия $a$ в состоянии $s$:\n", "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n", "$$V_{(i+1)}(s) = \\max_a Q_i(s,a)$$\n", "\n", "Зададим напрямую модель MDP с картинки:\n", "<img src=\"https://camo.githubusercontent.com/44c1a4d0063e3954f9eb6686a5b7b9763dadd54a/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f612f61642f4d61726b6f765f4465636973696f6e5f50726f636573732e737667\" caption=\"Марковский процесс принятия решений\" style=\"width: 400px;\">"]}, {"cell_type": "code", "execution_count": 180, "metadata": {}, "outputs": [], "source": ["try:\n", "    import colab\n", "    COLAB = True\n", "except ModuleNotFoundError:\n", "    COLAB = False\n", "    pass\n", "\n", "if COLAB:\n", "    !wget https://raw.githubusercontent.com/Tviskaron/mipt/master/2019/RL/02/mdp.py"]}, {"cell_type": "code", "execution_count": 181, "metadata": {"colab": {}, "colab_type": "code", "id": "2ebF4HaPsN48"}, "outputs": [], "source": ["transition_probs = {\n", "  's0':{\n", "    'a0': {'s0': 0.5, 's2': 0.5},\n", "    'a1': {'s2': 1}\n", "  },\n", "  's1':{\n", "    'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n", "    'a1': {'s1': 0.95, 's2': 0.05}\n", "  },\n", "  's2':{\n", "    'a0': {'s0': 0.4, 's2': 0.6},\n", "    'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n", "  }\n", "}\n", "rewards = {\n", "  's1': {'a0': {'s0': +5}},\n", "  's2': {'a1': {'s0': -1}}\n", "}\n", "\n", "from mdp import MDP\n", "import numpy as np\n", "mdp = MDP(transition_probs, rewards, initial_state='s0')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Теперь мы можем использовать это MDP, как и любое другое gym окружение:"]}, {"cell_type": "code", "execution_count": 182, "metadata": {}, "outputs": [], "source": ["state = mdp.reset()\n", "print('initial state =', state)\n", "next_state, reward, done, info = mdp.step('a1')\n", "print(f'next_state ={next_state}, reward = {reward}, done = {done}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Также, помимо стандартных методов, есть дополнительные, которые пригодятся нам для реализации метода итерации по полезностям."]}, {"cell_type": "code", "execution_count": 183, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 107}, "colab_type": "code", "id": "0bcgBOZbsN5A", "outputId": "9ff86175-e1cd-4d8b-d5ec-72f1cfb23cd4"}, "outputs": [], "source": ["print(\"all_states =\", mdp.get_all_states())\n", "print(\"possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n", "print(\"next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n", "print(\"reward('s1', 'a0', 's0') = \",mdp.get_reward('s1', 'a0', 's0'))\n", "print(\"transition_prob('s1', 'a0', 's0') = \",\n", "      mdp.get_transition_prob('s1', 'a0', 's0'))"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "WYDjsDv2sN5F"}, "source": ["### Задание 1\n", "\n", "Теперь реализуем алгоритм итерации по полезностям, чтобы решить этот вручную заданный MDP. Псевдокод алгоритма:\n", "\n", "---\n", "\n", "`1.` Инициализируем $V^{(0)}(s)=0$, для всех $s$\n", "\n", "`2.` For $i=0, 1, 2, \\dots$\n", " \n", "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, для всех $s$\n", "\n", "---\n", "\n", "Вначале вычисляем оценку состояния-действия:\n", "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$"]}, {"cell_type": "code", "execution_count": 184, "metadata": {"colab": {}, "colab_type": "code", "id": "zVSz8FCRsN5G"}, "outputs": [], "source": ["def get_action_value(mdp, state_values, state, action, \n", "                     gamma):\n", "    \"\"\" Вычислеям Q(s,a) по формуле выше \"\"\"\n", "    # вычислеяем оценку состояния\n", "    # Q = \n", "    ####### Здесь ваш код ########      \n", "    raise NotImplementedError      \n", "    ##############################\n", "    \n", "    return Q"]}, {"cell_type": "code", "execution_count": 185, "metadata": {"colab": {}, "colab_type": "code", "id": "U8rRYvrpsN5I"}, "outputs": [], "source": ["test_Vs = {s: i for i, s in enumerate(sorted(mdp.get_all_states()))}\n", "assert np.isclose(get_action_value(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)\n", "assert np.isclose(get_action_value(mdp, test_Vs, 's1', 'a0', 0.9), 3.95)"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "1tEz93sasN5K"}, "source": ["Теперь оцениваем полезность самого состояния, для этого мы можем использовать предыдущий метод:\n", "\n", "$$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"]}, {"cell_type": "code", "execution_count": 186, "metadata": {"colab": {}, "colab_type": "code", "id": "QA8c-2_fsN5K"}, "outputs": [], "source": ["def get_new_state_value(mdp, state_values, state, gamma):\n", "    \"\"\" Считаем следующее V(s) по формуле выше.\"\"\"\n", "    if mdp.is_terminal(state): \n", "        return 0\n", "    # V = \n", "    ####### Здесь ваш код ######## \n", "    raise NotImplementedError \n", "    ##############################\n", "    \n", "    return V"]}, {"cell_type": "code", "execution_count": 187, "metadata": {"colab": {}, "colab_type": "code", "id": "ym4og12hsN5N"}, "outputs": [], "source": ["test_Vs_copy = dict(test_Vs)\n", "assert np.isclose(get_new_state_value(mdp, test_Vs, 's0', 0.9), 1.8)\n", "assert np.isclose(get_new_state_value(mdp, test_Vs, 's2', 0.9), 1.08)\n", "assert np.isclose(get_new_state_value(mdp, {'s0': -1e10, 's1': 0, 's2': -2e10}, 's0', 0.9), -13500000000.0), \\\n", "   \"Убедитесь, что вы правильно обрабатываете отрицательные значения Q произвольной величины.\"\n", "assert test_Vs == test_Vs_copy, \"Убедитесь, что вы не изменяете state_values в функции get_new_state_value\""]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "emA2sv-QsN5P"}, "source": ["Теперь создаем основной цикл итерационного оценки полезности состояний с критерием остановки, который проверяет насколько изменились полезности."]}, {"cell_type": "code", "execution_count": 188, "metadata": {"colab": {}, "colab_type": "code", "id": "er9KwtjBsN5Q"}, "outputs": [], "source": ["def value_iteration(mdp, state_values=None,\n", "    gamma = 0.9, num_iter = 1000, min_difference = 1e-5):\n", "    \"\"\" выполняет num_iter шагов итерации по значениям\"\"\"\n", "    # инициализируем V(s)\n", "    state_values = state_values or \\\n", "    {s : 0 for s in mdp.get_all_states()}\n", "    \n", "    for i in range(num_iter):\n", "        # Вычисляем новые полезности состояний, \n", "        # используя функции, определенные выше. \n", "        # Должен получиться словарь {s: new_V(s)}\n", "        # new_state_values = \n", "        ####### Здесь ваш код ########\n", "        raise NotImplementedError\n", "        ##############################\n", "        \n", "        assert isinstance(new_state_values, dict)\n", "\n", "        # Считаем разницу\n", "        diff =  max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n", "        \n", "        print(\"iter %4i | diff: %6.5f | V(start): %.3f \"%\n", "          (i, diff, new_state_values[mdp._initial_state]))\n", "        \n", "        state_values = new_state_values\n", "        if diff < min_difference:\n", "            print(\"Принято! Алгоритм сходится!\")\n", "            break\n", "            \n", "    return state_values\n", "\n", "state_values = value_iteration(mdp,\n", "        num_iter = 100, min_difference = 0.001)"]}, {"cell_type": "code", "execution_count": 189, "metadata": {"colab": {}, "colab_type": "code", "id": "3cKoN2-tsN5V"}, "outputs": [], "source": ["print(\"Final state values:\", state_values)\n", "\n", "assert abs(state_values['s0'] - 3.781) < 0.01\n", "assert abs(state_values['s1'] - 7.294) < 0.01\n", "assert abs(state_values['s2'] - 4.202) < 0.01"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "bSIBT-49sN5X"}, "source": ["По найденным полезностям и зная модель переходов легко найти оптимальную стратегию:\n", "$$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$"]}, {"cell_type": "code", "execution_count": 190, "metadata": {"colab": {}, "colab_type": "code", "id": "x9m7NArGsN5Y"}, "outputs": [], "source": ["def get_optimal_action(mdp, state_values, state, \n", "                       gamma=0.9):\n", "    \"\"\" Находим оптимальное действие, используя формулу выше. \"\"\"\n", "    if mdp.is_terminal(state): return None\n", "    \n", "    actions = mdp.get_possible_actions(state)\n", "    # выбираем лучшее действие\n", "    # i = \n", "    ####### Здесь ваш код ########\n", "    raise NotImplementedError\n", "    ##############################\n", "    \n", "    return actions[i]"]}, {"cell_type": "code", "execution_count": 191, "metadata": {"colab": {}, "colab_type": "code", "id": "tCt9IjLnsN5Z"}, "outputs": [], "source": ["assert get_optimal_action(mdp, state_values, 's0', 0.9) == 'a1'\n", "assert get_optimal_action(mdp, state_values, 's1', 0.9) == 'a0'\n", "assert get_optimal_action(mdp, state_values, 's2', 0.9) == 'a1'\n", "\n", "assert get_optimal_action(mdp, {'s0': -1e10, 's1': 0, 's2': -2e10}, 's0', 0.9) == 'a0', \\\n", "    \"Убедитесь, что вы правильно обрабатываете отрицательные значения Q произвольной величины.\"\n", "assert get_optimal_action(mdp, {'s0': -2e10, 's1': 0, 's2': -1e10}, 's0', 0.9) == 'a1', \\\n", "    \"Убедитесь, что вы правильно обрабатываете отрицательные значения Q произвольной величины.\""]}, {"cell_type": "code", "execution_count": 192, "metadata": {}, "outputs": [], "source": ["# Проверим среднее вознаграждение агента\n", "\n", "s = mdp.reset()\n", "rewards = []\n", "for _ in range(10000):\n", "    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, 0.9))\n", "    rewards.append(r)\n", "\n", "print(\"average reward: \", np.mean(rewards))\n", "\n", "assert(0.40 < np.mean(rewards) < 0.55)"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "-omPxnyksN5b"}, "source": ["### Задание 2\n", "\n", "Теперь проверим работу итерации по ценностям на классической задаче FrozenLake."]}, {"cell_type": "code", "execution_count": 193, "metadata": {"colab": {}, "colab_type": "code", "id": "QhBAOCyEsN5c"}, "outputs": [], "source": ["from mdp import FrozenLakeEnv\n", "mdp = FrozenLakeEnv(slip_chance=0)\n", "\n", "mdp.render()\n", "state_values = value_iteration(mdp)"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "-kS3QDqNsN5h"}, "source": ["Визуализируем нашу стратегию."]}, {"cell_type": "code", "execution_count": 194, "metadata": {"colab": {}, "colab_type": "code", "id": "wRbL3izdsN5h"}, "outputs": [], "source": ["def draw_policy(mdp, state_values, gamma=0.9):\n", "    \"\"\"функция визуализации стратегии\"\"\"\n", "    plt.figure(figsize=(3, 3))\n", "    h, w = mdp.desc.shape\n", "    states = sorted(mdp.get_all_states())\n", "    V = np.array([state_values[s] for s in states])\n", "    Pi = {\n", "        s: get_optimal_action(mdp, state_values, s, gamma)\n", "        for s in states}\n", "    plt.imshow(V.reshape(w, h),\n", "               cmap='gray', interpolation='none',\n", "               clim=(0, 1))\n", "    ax = plt.gca()\n", "    ax.set_xticks(np.arange(h) - .5)\n", "    ax.set_yticks(np.arange(w) - .5)\n", "    ax.set_xticklabels([])\n", "    ax.set_yticklabels([])\n", "    Y, X = np.mgrid[0:4, 0:4]\n", "    a2uv = {'left': (-1, 0), 'down': (0, -1),\n", "            'right': (1, 0), 'up': (-1, 0)}\n", "    for y in range(h):\n", "        for x in range(w):\n", "            plt.text(x, y, str(mdp.desc[y, x].item()),\n", "                     color='g', size=12,\n", "                     verticalalignment='center',\n", "                     horizontalalignment='center',\n", "                     fontweight='bold')\n", "            a = Pi[y, x]\n", "            if a is None: continue\n", "            u, v = a2uv[a]\n", "            plt.arrow(x, y, u * .3, -v * .3,\n", "                      color='m', head_width=0.1,\n", "                      head_length=0.1)\n", "    plt.grid(color='b', lw=2, ls='-')\n", "    plt.show()\n"]}, {"cell_type": "code", "execution_count": 195, "metadata": {"colab": {}, "colab_type": "code", "id": "lQiqx1QesN5k"}, "outputs": [], "source": ["from IPython.display import clear_output\n", "from time import sleep\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "mdp = FrozenLakeEnv(map_name='8x8',slip_chance=0.1)\n", "state_values = {s : 0 for s in mdp.get_all_states()}\n", "\n", "for i in range(30):\n", "    clear_output(True)\n", "    print(\"after iteration %i\"%i)\n", "    state_values = value_iteration(mdp, \n", "                            state_values, num_iter=1)\n", "    draw_policy(mdp, state_values)\n", "    sleep(0.5)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Посмотрим на оптимальную стратегию:"]}, {"cell_type": "code", "execution_count": 196, "metadata": {}, "outputs": [], "source": ["s = mdp.reset()\n", "mdp.render()\n", "for t in range(100):\n", "    a = get_optimal_action(mdp, state_values, s, 0.9)\n", "    print(a, end='\\n\\n')\n", "    s, r, done, _ = mdp.step(a)\n", "    mdp.render()\n", "    if done:\n", "        break"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "62xktmhWsN5n"}, "source": ["Тестируем на более сложном варианте окружения:"]}, {"cell_type": "code", "execution_count": 197, "metadata": {"colab": {}, "colab_type": "code", "id": "BZsHbVNzsN5o"}, "outputs": [], "source": ["mdp = FrozenLakeEnv(slip_chance=0.2, map_name='8x8')\n", "state_values = value_iteration(mdp)\n", "\n", "total_rewards = []\n", "for game_i in range(1000):\n", "    s = mdp.reset()\n", "    rewards = []\n", "    for t in range(100):\n", "        # выполняем оптимальное действие в окружении\n", " \n", "        s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, 0.9))\n", "       \n", "        rewards.append(r)\n", "        if done: break\n", "    total_rewards.append(np.sum(rewards))\n", "    \n", "print(\"Cреднее вознаграждение:\", np.mean(total_rewards))\n", "assert(0.6 <= np.mean(total_rewards) <= 0.8)\n", "print(\"Принято!\")"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "xdMVQ1XQsN5s"}, "source": ["### Задание 3\n", "\n", "Теперь рассмотрим алгоритм итерации по стратегиям (PI, policy iteration):\n", "\n", "---\n", "Initialize $\\pi_0$   `// случайно`\n", "\n", "For $n=0, 1, 2, \\dots$\n", "- Считаем функцию $V^{\\pi_{n}}$\n", "- Используя $V^{\\pi_{n}}$, считаем функцию $Q^{\\pi_{n}}$\n", "- Получаем новую стратегию: $\\pi_{n+1}(s) = \\operatorname*{argmax}_a Q^{\\pi_{n}}(s,a)$\n", "---\n", "\n", "PI включает в себя оценку полезности состояния, как внутренний шаг."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "7Pg53dh5sN5s"}, "source": ["Вначале оценим полезности, используя текущую стратегию:\n", "$$V^{\\pi}(s) = \\sum_{s'} P(s,\\pi(s),s')[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s')]$$\n", "    Мы будем искать точное решение, хотя могли использовать и предыдущий итерационный подход. Для этого будем решать систему линейных уравнений относительно $V^{\\pi}(s_i)$ с помощью np.linalg.solve."]}, {"cell_type": "code", "execution_count": 198, "metadata": {"colab": {}, "colab_type": "code", "id": "sP2gL08osN5u"}, "outputs": [], "source": ["from numpy.linalg import solve\n", "\n", "def compute_vpi(mdp, policy, gamma):\n", "    \"\"\"\n", "    Считем V^pi(s) для всех состояний, согласно стратегии.\n", "    :param policy: словарь состояние->действие {s : a}\n", "    :returns: словарь {state : V^pi(state)}\n", "    \"\"\"\n", "    states = mdp.get_all_states()\n", "    A, b = [], []\n", "    for i, state in enumerate(states):\n", "        if state in policy:\n", "            a = policy[state]\n", "            # формируем матрицу A (... A.append(...))\n", "            ####### Здесь ваш код ########             \n", "            raise NotImplementedError             \n", "            ##############################\n", "            \n", "            # и вектор b (b.append(...))\n", "            ####### Здесь ваш код ########\n", "            raise NotImplementedError\n", "            ##############################\n", "            \n", "        else:\n", "            # формируем матрицу A (... A.append(...))\n", "            ####### Здесь ваш код ########\n", "            raise NotImplementedError\n", "            ##############################\n", "            \n", "            # вектор b (b.append(...))\n", "            ####### Здесь ваш код ########\n", "            raise NotImplementedError\n", "            ##############################\n", "            \n", "    A = np.array(A)\n", "    b = np.array(b)\n", "    \n", "    values = solve(A, b)\n", "    \n", "    state_values = {states[i] : values[i] \n", "                    for i in range(len(states))}\n", "    return state_values"]}, {"cell_type": "code", "execution_count": 199, "metadata": {"colab": {}, "colab_type": "code", "id": "1aG9dcJVsN5w"}, "outputs": [], "source": ["transition_probs = {\n", "    's0': {\n", "        'a0': {'s0': 0.5, 's2': 0.5},\n", "        'a1': {'s2': 1}\n", "    },\n", "    's1': {\n", "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n", "        'a1': {'s1': 0.95, 's2': 0.05}\n", "    },\n", "    's2': {\n", "        'a0': {'s0': 0.4, 's1': 0.6},\n", "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n", "    }\n", "}\n", "rewards = {\n", "    's1': {'a0': {'s0': +5}},\n", "    's2': {'a1': {'s0': -1}}\n", "}\n", "mdp = MDP(transition_probs, rewards, initial_state='s0')\n", "\n", "gamma = 0.9\n", "\n", "test_policy = {\n", "    s: np.random.choice(mdp.get_possible_actions(s))\n", "    for s in mdp.get_all_states()}\n", "new_vpi = compute_vpi(mdp, test_policy, gamma)\n", "\n", "print(new_vpi)\n", "assert type(new_vpi) is dict, \\\n", "    \"функция compute_vpi должна возвращать словарь \\\n", "    {состояние s : V^pi(s) }\""]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "hD-bBuwKsN5y"}, "source": ["Теперь обновляем стратегию на основе новых значений полезностей:"]}, {"cell_type": "code", "execution_count": 200, "metadata": {"colab": {}, "colab_type": "code", "id": "5pKbE1visN50"}, "outputs": [], "source": ["def compute_new_policy(mdp, vpi, gamma):\n", "    \"\"\"\n", "    Рассчитываем новую стратегию\n", "    :param vpi: словарь {state : V^pi(state) }\n", "    :returns: словарь {state : оптимальное действие}\n", "    \"\"\"\n", "    Q = {}\n", "    for state in mdp.get_all_states():\n", "        Q[state] = {}\n", "        for a in mdp.get_possible_actions(state):\n", "            values = []\n", "            for next_state in mdp.get_next_states(state,\n", "                                                  a):\n", "                r = mdp.get_reward(state, a, next_state)\n", "                p = mdp.get_transition_prob(state, a,\n", "                                            next_state)\n", "                # values.append(...)\n", "                ####### Здесь ваш код ########\n", "                raise NotImplementedError\n", "                ##############################\n", "                \n", "            Q[state][a] = sum(values)\n", "\n", "    policy = {}\n", "    for state in mdp.get_all_states():\n", "        actions = mdp.get_possible_actions(state)\n", "        if actions:\n", "            # выбираем оптимальное действие в state\n", "            # policy[state] = ... \n", "            ####### Здесь ваш код ########\n", "            raise NotImplementedError\n", "            ##############################\n", "            \n", "    return policy"]}, {"cell_type": "code", "execution_count": 201, "metadata": {"colab": {}, "colab_type": "code", "id": "SKkZnfGRsN53"}, "outputs": [], "source": ["new_policy = compute_new_policy(mdp, new_vpi, gamma)\n", "\n", "print(new_policy)\n", "\n", "assert type(new_policy) is dict, \\\n", "\"функция compute_new_policy должна возвращать словарь \\\n", "{состояние s: оптимальное действие}\""]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "XHax1qV5sN55"}, "source": ["Собираем все в единый цикл:"]}, {"cell_type": "code", "execution_count": 202, "metadata": {"colab": {}, "colab_type": "code", "id": "o4rfeoFasN56"}, "outputs": [], "source": ["def policy_iteration(mdp, policy=None, gamma = 0.9, \n", "                 num_iter = 1000, min_difference = 1e-5):\n", "    \"\"\" \n", "    Запускаем цикл итерации по стратегиям \n", "    Если стратегия не определена, задаем случайную\n", "    \"\"\"\n", "    for i in range(num_iter):\n", "        if not policy:\n", "            policy = {}\n", "            for s in mdp.get_all_states():\n", "                if mdp.get_possible_actions(s):\n", "                    policy[s] = np.random \\\n", "                    .choice(mdp.get_possible_actions(s))\n", "        # state_values = \n", "        ####### Здесь ваш код ########\n", "        raise NotImplementedError\n", "        ##############################\n", "        \n", "        #policy =\n", "        ####### Здесь ваш код ########\n", "        raise NotImplementedError\n", "        ##############################        \n", "    return state_values, policy"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "NJm34Gc5sN57"}, "source": ["Тестируем на FrozenLake."]}, {"cell_type": "code", "execution_count": 20, "metadata": {"colab": {}, "colab_type": "code", "id": "NC7hXWhssN58"}, "outputs": [], "source": ["mdp = FrozenLakeEnv(slip_chance=0.1)\n", "state_values, policy = policy_iteration(mdp)\n", "\n", "total_rewards = []\n", "for game_i in range(1000):\n", "    s = mdp.reset()\n", "    rewards = []\n", "    for t in range(100):\n", "        s, r, done, _ = mdp.step(policy[s])\n", "        rewards.append(r)\n", "        if done: break\n", "    total_rewards.append(np.sum(rewards))\n", "    \n", "print(\"average reward: \", np.mean(total_rewards))\n", "assert(0.8 <= np.mean(total_rewards) <= 0.95)\n", "print(\"Принято!\")"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["\n", "\n", ";"]}], "metadata": {"colab": {"name": "sem2_dp.ipynb", "provenance": [], "version": "0.3.2"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 1}