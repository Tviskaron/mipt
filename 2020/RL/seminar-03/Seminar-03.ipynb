{"cells": [{"cell_type": "markdown", "metadata": {"id": "TDqfgGd0bL3C"}, "source": ["# Машинное обучение с подкреплением. МФТИ: МТИИ 2020-2021.\n", "## Семинар 3: Обзор сред. Q-обучение. Аппроксимация Q-функции. \n", "\n", "### 1. Обзор сред\n", "\n", "* [Classic Control](https://gym.openai.com/envs/#classic_control)\n", "* [Box2D](https://gym.openai.com/envs/#box2d)\n", "* [Atari](https://gym.openai.com/envs/#atari)\n", "* [Gym Retro](https://openai.com/blog/gym-retro/)\n", "* [Mujoco](https://gym.openai.com/envs/#mujoco)\n", "* [Robotics](https://gym.openai.com/envs/#robotics)\n", "* [Universe](https://openai.com/blog/universe/)\n", "* [MineRL](https://minerl.io/) \\(использует проект [malmo](https://www.microsoft.com/en-us/research/project/project-malmo/))\n", "* [Starcraft II](https://github.com/deepmind/pysc2)\n", "* [Biomechanics: Learning to move](https://www.aicrowd.com/challenges/neurips-2019-learning-to-move-walk-around)\n", "* [Procgen](https://openai.com/blog/procgen-benchmark/)\n", "* [Halitate on Kaggle](https://www.kaggle.com/c/halite) \n", "* [Flatland](https://www.aicrowd.com/challenges/neurips-2020-flatland-challenge)\n", "* Настольные игры: Chess, GO и т.д. (множество среда на github)\n", "* [Learning to Run a Power Network](https://competitions.codalab.org/competitions/20767) \\(ссылка на [NIPS](https://nips.cc/Conferences/2020/CompetitionTrack))\n", "* ..."]}, {"cell_type": "markdown", "metadata": {"id": "ajUaHPOJH-3Z"}, "source": ["### 2. Q-обучение\n"]}, {"cell_type": "markdown", "metadata": {"id": "fIX-e9zJKk23"}, "source": ["Одним из наиболее популярных алгоритм обучения на основе временных различий является Q-обучение.Уравнение Беллмана для значения Q-функции записывается как:\n", "\n", "$$Q(s,a)=r(s)+\\gamma\\sum_s'T(s,a,s')\\max_{a'}Q(a',s')$$\n", "\n", "Уравнение для итерационного обновления значений Q-функции выглядит следующим образом:$$Q(s,a)\\leftarrow Q(s,a)+\\alpha \\big (r(s)+\\gamma\\max_{a'}Q(a',s') - Q(s,a) \\big ).$$\n", "\n", "Раскроем скобки:\n", "$$Q(s,a)\\leftarrow (1 - \\alpha) \\times Q(s,a)+\\alpha \\times \\big (r(s)+\\gamma\\max_{a'}Q(a',s')\\big ).$$\n", "\n", "Ничего не напоминает?"]}, {"cell_type": "markdown", "metadata": {"id": "WJ79cSTLLPb-"}, "source": ["Для обучения будем использовать среду Taxi-v3. Подробнее про данное окружение можно посмотреть в документации: https://gym.openai.com/envs/Taxi-v3/."]}, {"cell_type": "code", "execution_count": 7, "metadata": {"id": "ud0_kfOGLUwo"}, "outputs": [], "source": ["import gym\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import random"]}, {"cell_type": "code", "execution_count": 8, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 153}, "id": "uwqXBprJKk26", "outputId": "1f68c1c4-3d4b-494e-9bc2-e3eb7d1d7d4f"}, "outputs": [], "source": ["env = gym.make(\"Taxi-v3\")\n", "env.render()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": 9, "metadata": {"id": "Phe3a5aTQBsd"}, "outputs": [], "source": ["def show_progress(rewards_batch, log, reward_range=None):\n", "    \"\"\"\n", "    Удобная функция, которая отображает прогресс обучения.\n", "    \"\"\"\n", "\n", "    if reward_range is None:\n", "        reward_range = [-990, +10]\n", "    mean_reward = np.mean(rewards_batch)\n", "    log.append([mean_reward])\n", "\n", "    clear_output(True)\n", "    plt.figure(figsize=[8, 4])\n", "    plt.subplot(1, 2, 1)\n", "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n", "    plt.legend(loc=4)\n", "    plt.grid()\n", "    plt.grid()\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "m_Pbvpm4Kk2-"}, "source": ["#### Задание 1\n", "\n", "Создайте таблицу из нулей, используя информацию из окружения о количестве состояний и действий"]}, {"cell_type": "code", "execution_count": 10, "metadata": {"id": "_bddYTsDQmVL"}, "outputs": [], "source": ["import random\n", "from IPython.display import clear_output\n", "\n", "# гиперпараметры алгоритма\n", "alpha = 0.1\n", "gamma = 0.9\n", "epsilon = 0.1\n", "episodes_number = 10001"]}, {"cell_type": "code", "execution_count": 11, "metadata": {"id": "8QjwLO_CKk3B"}, "outputs": [], "source": ["def initialize_q_table(observation_space_n, action_space_n):\n", "    # подсказка смотрим env.observation_space и env.action_space\n", "    # q_table_ = [state][action]\n", "    ####### Здесь ваш код ########\n", "    raise NotImplementedError\n", "    ##############################\n", "    return q_table_"]}, {"cell_type": "markdown", "metadata": {"id": "pG9YEmftQtu0"}, "source": ["#### Задание 2\n", "\n", "Напишите код для формулы Q-обновления, используя известные: alpha, reward, gamma, next_max, old_value (q_table[state, action])"]}, {"cell_type": "code", "execution_count": 12, "metadata": {"id": "DUthRLiaIuuV"}, "outputs": [], "source": ["# определяем память, в которой будет храниться Q(s,a)\n", "q_table = initialize_q_table(env.observation_space.n, env.action_space.n)\n", "log = []\n", "rewards_batch = []\n", "\n", "for i in range(1, episodes_number):\n", "    state = env.reset()\n", "\n", "    episode, reward, episode_reward = 0, 0, 0\n", "    done = False\n", "    \n", "    while not done:\n", "        # выбираем действие, используя eps-greedy исследование среды\n", "        # с вероятностью epsilon выбираем случайное действие, иначе \n", "        # выполняем действие жадно, согласно текущей Q-таблице \n", "        # action = \n", "        ####### Здесь ваш код ########\n", "        raise NotImplementedError\n", "        ##############################\n", "        \n", "        # выполняем действие в среде \n", "        next_state, reward, done, info = env.step(action) \n", "        \n", "        # получаем old_value (Q(s,a)) и next_max (max(Q(s', a')))\n", "        old_value = q_table[state, action]\n", "        next_max = np.max(q_table[next_state])\n", "        \n", "        # код для Q-обновления\n", "        # new_value = \n", "        ####### Здесь ваш код ########\n", "        raise NotImplementedError\n", "        ##############################\n", "        \n", "        q_table[state, action] = new_value\n", "\n", "        state = next_state\n", "        episode += 1\n", "        episode_reward += reward\n", "    rewards_batch.append(episode_reward)\n", "     \n", "    if i % 100 == 0:\n", "        show_progress(rewards_batch, log)\n", "        rewards_batch = []\n", "        print(f\"Episode: {i}, Reward: {episode_reward}\")"]}, {"cell_type": "markdown", "metadata": {"id": "QCvfdLORQOf0"}, "source": ["### Интерпретация результатов:\n", "Если все сделано правильно, то график должен выйти на плато около 0. Значение вознаграждение будет в диапазоне [-5, 10], за счет случайного выбора начальной позиции такси и пассажира. Попробуйте изменить гиперпараметры и сравните результаты."]}, {"cell_type": "markdown", "metadata": {"id": "f7sfWBeBq8Wx"}, "source": ["## 3. Аппроксимация Q-функции\n", "\n", "В данном пункте мы будем использовать библиотеку pytorch для обучения нейронной сети, хотя можно использовать и любую другую библиотеку."]}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [], "source": ["import gym\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline"]}, {"cell_type": "code", "execution_count": 47, "metadata": {"id": "YRnOxiAZrOFN"}, "outputs": [], "source": ["env = gym.make(\"CartPole-v0\").env\n", "\n", "env.reset()\n", "n_actions = env.action_space.n\n", "state_dim = env.observation_space.shape\n", "\n", "# plt.imshow(env.render(\"rgb_array\"))\n", "env.close()"]}, {"cell_type": "markdown", "metadata": {"id": "cYbIV7w42Fp1"}, "source": ["Так как описание состояния в задаче с маятником представляет собой не \"сырые\" признаки, а уже предобработанные (координаты, углы), нам не нужна для начала сложная архитектура, начнем с такой:\n", "<img src=\"https://raw.githubusercontent.com/Tviskaron/mipt/master/2020/RL/figures/DQN.svg\">\n", "Для начала попробуйте использовать только полносвязные слои (``torch.nn.Linear``) и простые активационные функции (``torch.nn.ReLU``). Сигмоиды и другие функции не будут работать с ненормализованными входными данными."]}, {"cell_type": "code", "execution_count": 29, "metadata": {"id": "5BFkc4eN16Lh"}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Определяем граф вычислений:"]}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [], "source": ["# network = nn.Sequential(\n", "# torch.nn.Linear(state_dim[0], ...),\n", "# ...\n", "####### Здесь ваш код ########\n", "raise NotImplementedError\n", "##############################"]}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [], "source": ["def get_action(state, epsilon=0):\n", "    \"\"\"\n", "    сэмплируем (eps greedy) действие\n", "    \"\"\"\n", "    state = torch.tensor(state[None], dtype=torch.float32)\n", "    q_values = network(state).detach().numpy()\n", "    \n", "    # action = \n", "    ####### Здесь ваш код ########\n", "    raise NotImplementedError\n", "    ##############################\n", "    \n", "    return int(action)"]}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [], "source": ["s = env.reset()\n", "assert tuple(network(torch.tensor([s]*3, dtype=torch.float32)).size()) == (\n", "    3, n_actions), \"Убедитесь, что модель переводит s -> [Q(s,a0), ..., Q(s, a_last)]\"\n", "assert isinstance(list(network.modules(\n", "))[-1], nn.Linear), \"убедитесь, что вы предсказываете q без нелинейности\"\n", "assert isinstance(get_action(\n", "    s), int), \"убедитесь, что функция get_action() возвращает только одно действие типа integer\" % (type(get_action(s)))\n", "\n", "# проверяем исследование среды\n", "for eps in [0., 0.1, 0.5, 1.0]:\n", "    state_frequencies = np.bincount(\n", "        [get_action(s, epsilon=eps) for i in range(10000)], minlength=n_actions)\n", "    best_action = state_frequencies.argmax()\n", "    assert abs(state_frequencies[best_action] -\n", "               10000 * (1 - eps + eps / n_actions)) < 200\n", "    for other_action in range(n_actions):\n", "        if other_action != best_action:\n", "            assert abs(state_frequencies[other_action] -\n", "                       10000 * (eps / n_actions)) < 200\n", "    print('e=%.1f тесты пройдены' % eps)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Теперь будем приближать Q-функцию агента, минимизируя TD функцию потерь:\n", "$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2,$$\n", "где\n", "* $s, a, r, s'$ состояние, действие, вознаграждение и следующее состояние \n", "* $\\gamma$ дисконтирующий множетель.\n", "\n", "Основная тонкость состоит в использовании $Q_{-}(s',a')$. Эта та же самая функция, что и $Q_{\\theta}$, которая является выходом нейронной сети, но при обучении сети, мы не пропускаем через эти слои градиенты. "]}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [], "source": ["def compute_td_loss(states, actions, rewards, next_states, is_done, gamma=0.99, check_shapes=False):\n", "    \"\"\" Считатет td ошибку, используя лишь операции фреймворка torch. Используйте формулу выше. \"\"\"\n", "    \n", "    # переводим входные данные в тензоры\n", "    states = torch.tensor(states, dtype=torch.float32)    # shape: [batch_size, state_size]\n", "    actions = torch.tensor(actions, dtype=torch.long)     # shape: [batch_size]\n", "    rewards = torch.tensor(rewards, dtype=torch.float32)  # shape: [batch_size]\n", "    \n", "    \n", "    next_states = torch.tensor(next_states, dtype=torch.float32) # shape: [batch_size, state_size]\n", "    is_done = torch.tensor(is_done, dtype=torch.uint8)    # shape: [batch_size]\n", "\n", "    # получаем значения q для всех действий из текущих состояний\n", "    predicted_qvalues = network(states)\n", "\n", "    # получаем q-values для выбранных действий\n", "    predicted_qvalues_for_actions = predicted_qvalues[range(states.shape[0]), actions]\n", "\n", "    # применяем сеть для получения q-value для следующих состояний (next_states)\n", "    # predicted_next_qvalues =\n", "    ####### Здесь ваш код ########\n", "    raise NotImplementedError\n", "    ##############################\n", "    \n", "    # вычисляем V*(next_states), что соответствует max_{a'} Q(s',a')\n", "    # next_state_values =\n", "    ####### Здесь ваш код ########\n", "    raise NotImplementedError\n", "    ##############################\n", "    \n", "    assert next_state_values.dtype == torch.float32\n", "\n", "    # вычисляем target q-values для функции потерь\n", "    #  target_qvalues_for_actions =\n", "    ####### Здесь ваш код ########\n", "    raise NotImplementedError\n", "    ##############################\n", "    \n", "    # для последнего действия используем \n", "    # упрощенную формулу Q(s,a) = r(s,a), \n", "    # т.к. s' для него не существует\n", "    target_qvalues_for_actions = torch.where(is_done, rewards, target_qvalues_for_actions)\n", "\n", "    # MSE loss для минимизации\n", "    loss = torch.mean((predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2)\n", "\n", "    if check_shapes:\n", "        assert predicted_next_qvalues.data.dim(\n", "        ) == 2, \"убедитесь, что вы предсказали q-значения для всех действий в следующем состоянии\"\n", "        assert next_state_values.data.dim(\n", "        ) == 1, \"убедитесь, что вы вычислили V (s ') как максимум только по оси действий, а не по всем осям\"\n", "        assert target_qvalues_for_actions.data.dim(\n", "        ) == 1, \"что-то не так с целевыми q-значениями, они должны быть вектором\"\n", "\n", "    return loss"]}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "outputs": [], "source": ["# небольшие проверки\n", "s = env.reset()\n", "a = env.action_space.sample()\n", "next_s, r, done, _ = env.step(a)\n", "loss = compute_td_loss([s], [a], [r], [next_s], [done], check_shapes=True)\n", "loss.backward()\n", "\n", "assert len(loss.size()) == 0, \"функция должна вычислять скалярный loss - среднее по батчу\"\n", "assert np.any(next(network.parameters()).grad.detach().numpy() !=\n", "              0), \"loss должен быть дифференцируемым по весам сети\""]}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [], "source": ["opt = torch.optim.Adam(network.parameters(), lr=1e-4)"]}, {"cell_type": "code", "execution_count": 36, "metadata": {}, "outputs": [], "source": ["def generate_session(env, t_max=1000, epsilon=0, train=False):\n", "    \"\"\"генерация сессии и обучение\"\"\"\n", "    total_reward = 0\n", "    s = env.reset()\n", "\n", "    for t in range(t_max):\n", "        a = get_action(s, epsilon=epsilon)\n", "        next_s, r, done, _ = env.step(a)\n", "\n", "        if train:\n", "            opt.zero_grad()\n", "            compute_td_loss([s], [a], [r], [next_s], [done]).backward()\n", "            opt.step()\n", "\n", "        total_reward += r\n", "        s = next_s\n", "        if done:\n", "            break\n", "\n", "    return total_reward"]}, {"cell_type": "code", "execution_count": 37, "metadata": {}, "outputs": [], "source": ["epsilon = 0.5"]}, {"cell_type": "code", "execution_count": 39, "metadata": {}, "outputs": [], "source": ["for i in range(150):\n", "    session_rewards = [generate_session(env, epsilon=epsilon, train=True) for _ in range(100)]\n", "    print(\"Epoch: #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), epsilon))\n", "\n", "    epsilon *= 0.99\n", "    assert epsilon >= 1e-4, \"убедитесь, что epsilon не становится < 0\"\n", "\n", "    if np.mean(session_rewards) > 300:\n", "        print(\"Принято!\")\n", "        break"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Интерпретация результатов:\n", "\n", "Добро пожаловать в мир глубокого обучения с подкреплением! Не ждите, что вознаграждение агента будет увеличиваться плавно. Может быть оно начнет расти, если сочтет ваш код достойным :)\n", "\n", "А если серьезно,\n", "* __mean reward__ $-$ это среднее вознаграждение за игру. При правильной реализации оно может оставаться низким в течение первых 10 эпох, а затем начнет расти, и сойдется к ~50-100 эпохе, в зависимости от архитектуры сети.\n", "* Если со сходимость возникли проблемы $-$ попробуйте увеличить количество скрытых нейронов или обратите внимание на эпсилон.\n", "* __epsilon__ $-$ agent's willingness to explore. Если вы видите, что эпислон находится на уровне <0.01, до того, как агент достиг вознаграждения >= 200, установите первоначальное значение 0.1 - 0.5."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Посмотрим на результаты:\n", "\n", "Подключаем визуализацию:"]}, {"cell_type": "code", "execution_count": 40, "metadata": {}, "outputs": [], "source": ["try:\n", "    import colab\n", "    COLAB = True\n", "except ModuleNotFoundError:\n", "    COLAB = False\n", "    pass\n", "\n", "if COLAB:\n", "    !wget https://gist.githubusercontent.com/Tviskaron/4d35eabce2e057dd2ea49a00b00aaa41/raw/f1e25fc6ac6d8f11cb585559ce8b2ab9ffefd67b/colab_render.sh -O colab_render.sh -q\n", "    !sh colab_render.sh\n", "    !wget https://gist.githubusercontent.com/Tviskaron/d91decc1ca5f1b09af2f9f080011a925/raw/0d3474f65b4aea533996ee00edf99a37e4da5561/colab_render.py -O colab_render.py -q \n", "    import colab_render"]}, {"cell_type": "code", "execution_count": 41, "metadata": {}, "outputs": [], "source": ["# библиотеки и функции, которые потребуеются для показа видео\n", "\n", "import glob\n", "import io\n", "import base64\n", "from IPython import display as ipythondisplay\n", "from IPython.display import HTML\n", "from gym.envs.classic_control import rendering\n", "import matplotlib.pyplot as plt\n", "\n", "%matplotlib inline\n", "\n", "\n", "org_constructor = rendering.Viewer.__init__\n", "\n", "\n", "def constructor(self, *args, **kwargs):\n", "    org_constructor(self, *args, **kwargs)\n", "    self.window.set_visible(visible=False)\n", "\n", "\n", "rendering.Viewer.__init__ = constructor\n", "\n", "\n", "def show_video(folder=\"./video\"):\n", "    mp4list = glob.glob(folder + '/*.mp4')\n", "    if len(mp4list) > 0:\n", "        mp4 = sorted(mp4list, key=lambda x: x[-15:], reverse=True)[0]\n", "        video = io.open(mp4, 'r+b').read()\n", "        encoded = base64.b64encode(video)\n", "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n", "                loop controls style=\"height: 400px;\">\n", "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n", "             </video>'''.format(encoded.decode('ascii'))))\n", "    else:\n", "        print(\"Could not find video\")"]}, {"cell_type": "code", "execution_count": 44, "metadata": {}, "outputs": [], "source": ["env = gym.make(\"CartPole-v0\")\n", "env = gym.wrappers.Monitor(env, \"./video\", force=True)\n", "\n", "generate_session(env, epsilon=0, train=False)\n", "\n", "env.close()\n", "show_video()"]}, {"cell_type": "code", "execution_count": 46, "metadata": {}, "outputs": [], "source": ["\n", "\n", ""]}], "metadata": {"accelerator": "GPU", "colab": {"collapsed_sections": [], "name": "Seminar-02.ipynb", "provenance": []}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 1}
